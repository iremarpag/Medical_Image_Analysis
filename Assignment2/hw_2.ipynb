{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "medical_image_hw2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENR0SodQGtD_",
        "outputId": "98c6fed6-1038-41e8-919d-6c80ffcffafc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx1dHnfvG3OH"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "PROJECT_LOCATION = \"/content/drive/MyDrive/playground/medical_image_analysis/Homework2\"\n",
        "os.chdir(PROJECT_LOCATION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oQg-oygG5fW"
      },
      "source": [
        "DATA_LOCATION = \"./dataset/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9o_KmFuG76i"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow\n",
        "from skimage import io\n",
        "from skimage import color\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPBO-c15G8pz"
      },
      "source": [
        "def calculateCooccurenceMatrix(grayImg, binNumber, di, dj):\n",
        "  M = np.zeros((binNumber, binNumber))\n",
        "  bin_size = 256 // binNumber\n",
        "  bin_gray_img = grayImg // bin_size\n",
        "  new_shape = bin_gray_img.shape\n",
        "  shifted_bin_gray_img = np.zeros(bin_gray_img.shape)\n",
        "  shifted_bin_gray_img[:,:] = -1\n",
        "\n",
        "\n",
        "\n",
        "  if di > 0 and dj > 0:\n",
        "    shifted_bin_gray_img[:-di, :-dj] = bin_gray_img[di: ,dj:]\n",
        "  elif di > 0 and dj < 0:\n",
        "    shifted_bin_gray_img[:-di, -dj:] = bin_gray_img[di:, :dj]\n",
        "  elif di > 0 and dj == 0:\n",
        "    shifted_bin_gray_img[:-di, :] = bin_gray_img[di:, :]\n",
        "  elif di < 0 and dj > 0:\n",
        "    shifted_bin_gray_img[-di:, :-dj] = bin_gray_img[:di, dj:]\n",
        "  elif di < 0 and dj < 0:\n",
        "    shifted_bin_gray_img[-di:, -dj:] = bin_gray_img[:di, :dj]\n",
        "  elif di < 0 and dj == 0:\n",
        "    shifted_bin_gray_img[-di:, :] = bin_gray_img[:di, :]\n",
        "  elif di == 0 and dj == 0:\n",
        "    shifted_bin_gray_img = bin_gray_img\n",
        "  elif di == 0 and dj > 0:\n",
        "    shifted_bin_gray_img[:, :-dj] = bin_gray_img[:, dj:]\n",
        "  elif di == 0 and dj < 0:\n",
        "    shifted_bin_gray_img[:, -dj:] = bin_gray_img[:, :dj]\n",
        "  \n",
        "  \n",
        "\n",
        "  for i in range(binNumber):\n",
        "    for j in range(binNumber):\n",
        "      M[i, j] = np.sum(np.logical_and(bin_gray_img == i, shifted_bin_gray_img == j))\n",
        "\n",
        "  return M\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHUjH74tpTqP"
      },
      "source": [
        "def calculateAccumulatedCooccurrenceMatrix(grayImg, binNumber, d):\n",
        "  accM = np.zeros((binNumber, binNumber))\n",
        "  for i in [-d, 0, d]:\n",
        "    for j in [-d, 0, d]:\n",
        "      if i == 0 and j == 0:\n",
        "        continue\n",
        "      accM += calculateCooccurenceMatrix(grayImg, binNumber, i, j)\n",
        "  return accM\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAU2K0WB_ed9"
      },
      "source": [
        "def inverse_indifference_moment(n_acc_m):\n",
        "  n = n_acc_m.shape[0]\n",
        "  i = np.arange(1, n+1)\n",
        "  j = np.arange(1, n+1)\n",
        "  i.shape = (n, 1)\n",
        "  j.shape = (1, n)\n",
        "\n",
        "  return np.sum(n_acc_m / (1 + np.power(i - j, 2)))\n",
        "\n",
        "def contrast(n_acc_m):\n",
        "  n = n_acc_m.shape[0]\n",
        "  i = np.arange(1, n+1)\n",
        "  j = np.arange(1, n+1)\n",
        "  i.shape = (n, 1)\n",
        "  j.shape = (1, n)\n",
        "\n",
        "  return np.sum(n_acc_m * np.power(i - j, 2))\n",
        "\n",
        "\n",
        "def entropy(n_acc_m):\n",
        "  from numpy import ma\n",
        "  ma_n_acc_m = ma.log(n_acc_m).filled(0)\n",
        "  return - np.sum(n_acc_m * ma_n_acc_m)\n",
        "\n",
        "def correlation(n_acc_m):\n",
        "  n_i = np.sum(n_acc_m, 1)\n",
        "  n_j = np.sum(n_acc_m, 0)\n",
        "\n",
        "  n = n_acc_m.shape[0]\n",
        "  i = np.arange(1, n+1)\n",
        "  j = np.arange(1, n+1)\n",
        "\n",
        "  mu_x = np.sum(n_i * i)\n",
        "  mu_y = np.sum(n_j * j)\n",
        "  std_x = np.sqrt(np.sum(n_i * np.power(i - mu_x, 2)))\n",
        "  std_y = np.sqrt(np.sum(n_j * np.power(j - mu_y, 2)))\n",
        "\n",
        "  i.shape = (n, 1)\n",
        "  j.shape = (1, n)\n",
        "\n",
        "  sum = np.sum((i * j) * n_acc_m)\n",
        "\n",
        "  return (sum - mu_x * mu_y) / (std_x * std_y)\n",
        "\n",
        "def calculateCooccurrenceFeatures(accM):\n",
        "  normalized_acc_m = accM / np.sum(accM)\n",
        "  ang_nd_moment = np.sum(np.power(normalized_acc_m, 2))\n",
        "  max_prob = np.max(normalized_acc_m)\n",
        "  inv_ind_moment = inverse_indifference_moment(normalized_acc_m)\n",
        "  cont = contrast(normalized_acc_m)\n",
        "  ent = entropy(normalized_acc_m)\n",
        "  corr = correlation(normalized_acc_m)\n",
        "  return [ang_nd_moment, max_prob, inv_ind_moment, cont, ent, corr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiXSh92NeTPj"
      },
      "source": [
        "# get all images and their features\n",
        "bin_number, d = 8, 10\n",
        "training_dir = DATA_LOCATION + \"training/\"\n",
        "features = []\n",
        "for img_file in glob.glob(training_dir + \"*.jpg\"):\n",
        "  img = cv.imread(img_file, 0)\n",
        "  acc_m = calculateAccumulatedCooccurrenceMatrix(img, bin_number, d)\n",
        "  features.append(calculateCooccurrenceFeatures(acc_m))\n",
        "\n",
        "training_labels = np.loadtxt(DATA_LOCATION + \"training_labels.txt\")\n",
        "training_labels = training_labels.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbua80S0uhvt"
      },
      "source": [
        "test_dir = DATA_LOCATION + \"test/\"\n",
        "test_features = []\n",
        "for img_file in glob.glob(test_dir + \"*.jpg\"):\n",
        "  img = cv.imread(img_file, 0)\n",
        "  acc_m = calculateAccumulatedCooccurrenceMatrix(img, bin_number, d)\n",
        "  test_features.append(calculateCooccurrenceFeatures(acc_m))\n",
        "\n",
        "test_labels = np.loadtxt(DATA_LOCATION + \"test_labels.txt\")\n",
        "test_labels = test_labels.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1nzTQwiqpDK"
      },
      "source": [
        "Cs = [0.1, 1, 5, 10, 50, 100, 250, 500, 1000, 5000]\n",
        "gammas = [0.1, 1, 5, 10]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN-O8CmouEGs"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "scaler = StandardScaler()\n",
        "features_std = scaler.fit_transform(features)\n",
        "features_test_std = scaler.fit_transform(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfEVBVHKt-03"
      },
      "source": [
        "# linear SVM\n",
        "\n",
        "training_labels = np.array(training_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "features_std = np.array(features_std)\n",
        "features_test_std = np.array(features_test_std)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ai5-F8g87QP"
      },
      "source": [
        "for C in list(np.arange(5,10, 0.1)):\n",
        "  svc = SVC(kernel='linear', class_weight='balanced', C=C)\n",
        "  model = svc.fit(features_std, training_labels)\n",
        "  training_pred = np.array(model.predict(features_std))\n",
        "  test_pred = np.array(model.predict(features_test_std))\n",
        "  \n",
        "  print('C : ', C)\n",
        "  print('Training Overall Accuracy : ', accuracy_score(training_labels, training_pred))\n",
        "  print('Training Class Accuracy : ', confusion_matrix(training_labels, training_pred, normalize=\"true\").diagonal())\n",
        "  print('Test Overall Accuracy : ', accuracy_score(test_labels, test_pred))\n",
        "  print('Test Class Accuracy : ', confusion_matrix(test_labels, test_pred, normalize=\"true\").diagonal())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtZX78W9AxC6"
      },
      "source": [
        "for C in [1,5,10]:\n",
        "  for gamma in list(np.arange(0.1, 1, 0.1)):\n",
        "    svc = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
        "    model = svc.fit(features_std, training_labels)\n",
        "    training_pred = np.array(model.predict(features_std))\n",
        "    test_pred = np.array(model.predict(features_test_std))\n",
        "    \n",
        "    print('C : ', C, 'gamma : ', gamma)\n",
        "    print('Training Overall Accuracy : ', accuracy_score(training_labels, training_pred))\n",
        "    print('Training Class Accuracy : ', confusion_matrix(training_labels, training_pred, normalize=\"true\").diagonal())\n",
        "    print('Test Overall Accuracy : ', accuracy_score(test_labels, test_pred))\n",
        "    print('Test Class Accuracy : ', confusion_matrix(test_labels, test_pred, normalize=\"true\").diagonal())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex_ie6BBDVGE"
      },
      "source": [
        "# McNemar Linear and RBF comparison\n",
        "\n",
        "svc = SVC(kernel='linear', class_weight='balanced', C=5.4)\n",
        "model_linear = svc.fit(features_std, training_labels)\n",
        "training_pred_linear = np.array(model_linear.predict(features_std))\n",
        "test_pred_linear = np.array(model_linear.predict(features_test_std))\n",
        "\n",
        "\n",
        "\n",
        "svc = SVC(kernel='rbf', class_weight='balanced', C=10, gamma=0.1)\n",
        "model_rbf = svc.fit(features_std, training_labels)\n",
        "training_pred_rbf = np.array(model_rbf.predict(features_std))\n",
        "test_pred_rbf = np.array(model_rbf.predict(features_test_std))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BexUGKzqylju"
      },
      "source": [
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "\n",
        "def contingency(truth, m1, m2, val=0):\n",
        "  m = [[0,0],[0,0]]\n",
        "  tf_m1 = truth == m1\n",
        "  tf_m2 = truth == m2\n",
        "  if val != 0:\n",
        "    tf_m1 = tf_m1[truth == val]\n",
        "    tf_m2 = tf_m2[truth == val]\n",
        "  m[0][0] = np.sum(np.logical_and(tf_m1, tf_m2))\n",
        "  m[0][1] = np.sum(np.logical_and(tf_m1, np.logical_not(tf_m2)))\n",
        "  m[1][0] = np.sum(np.logical_and(np.logical_not(tf_m1), tf_m2))\n",
        "  m[1][1] = np.sum(np.logical_and(np.logical_not(tf_m1), np.logical_not(tf_m2)))\n",
        "  return m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSysLRNFEFWU",
        "outputId": "adf37b63-4884-47ce-b450-c3def254b660"
      },
      "source": [
        "for i in range(4):\n",
        "  table = contingency(training_labels, training_pred_linear, training_pred_rbf, i)\n",
        "  result = mcnemar(table, exact=False, correction=True)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pvalue      0.0093747684594349\n",
            "statistic   6.75\n",
            "pvalue      1.0\n",
            "statistic   0.0\n",
            "pvalue      0.02334220201289086\n",
            "statistic   5.142857142857143\n",
            "pvalue      0.47950012218695337\n",
            "statistic   0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIRPffVzJveF",
        "outputId": "592ff2b8-2090-4a6c-e5d6-1d8a7b3ac057"
      },
      "source": [
        "for i in range(4):\n",
        "  table = contingency(test_labels, test_pred_linear, test_pred_rbf, i)\n",
        "  result = mcnemar(table, exact=False, correction=True)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pvalue      1.0\n",
            "statistic   0.0\n",
            "pvalue      0.6170750774519739\n",
            "statistic   0.25\n",
            "pvalue      0.7236736098317629\n",
            "statistic   0.125\n",
            "pvalue      0.24821307898992026\n",
            "statistic   1.3333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2UqZBvZM6o9"
      },
      "source": [
        "# Part III\n",
        "N = 4\n",
        "# get all images and their features\n",
        "bin_number, d = 8, 10\n",
        "\n",
        "features_patches = []\n",
        "for img_file in glob.glob(training_dir + \"*.jpg\"):\n",
        "  img = cv.imread(img_file, 0)\n",
        "  x, y = img.shape\n",
        "  h, w = x // N, y // N\n",
        "  acc_m_list = []\n",
        "  for i in range(N):\n",
        "    for j in range(N):\n",
        "      patch = img[i*h:(i+1)*h, j*w:(j+1)*w]\n",
        "      acc_m = calculateAccumulatedCooccurrenceMatrix(patch, bin_number, d)\n",
        "      acc_m_list.append(calculateCooccurrenceFeatures(acc_m))\n",
        "\n",
        "  acc_m_np = np.array(acc_m_list)\n",
        "  features_patches.append(np.mean(acc_m_np, 0))\n",
        "    \n",
        "features_patches_np = np.array(features_patches)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB1kVxwiQk-L"
      },
      "source": [
        "test_features_patches = []\n",
        "\n",
        "for img_file in glob.glob(test_dir + \"*.jpg\"):\n",
        "  img = cv.imread(img_file, 0)\n",
        "  x, y = img.shape\n",
        "  h, w = x // N, y // N\n",
        "  acc_m_list = []\n",
        "  for i in range(N):\n",
        "    for j in range(N):\n",
        "      patch = img[i*h:(i+1)*h, j*w:(j+1)*w]\n",
        "      acc_m = calculateAccumulatedCooccurrenceMatrix(patch, bin_number, d)\n",
        "      acc_m_list.append(calculateCooccurrenceFeatures(acc_m))\n",
        "\n",
        "  acc_m_np = np.array(acc_m_list)\n",
        "  test_features_patches.append(np.mean(acc_m_np, 0))\n",
        "    \n",
        "test_features_patches_np = np.array(test_features_patches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPST2ZOuTfAr"
      },
      "source": [
        "Cs = [0.1, 1, 5, 10, 50, 100, 250, 500, 1000, 5000]\n",
        "gammas = [0.1, 1, 5, 10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi2_Cf7Qy8ah"
      },
      "source": [
        "features_patches_np_std = scaler.fit_transform(features_patches_np)\n",
        "test_features_patches_np_std = scaler.fit_transform(test_features_patches_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLwQBqYzzSkT"
      },
      "source": [
        "for C in np.arange(5, 10, 0.1):\n",
        "  svc = SVC(kernel='linear', class_weight='balanced', C=C)\n",
        "  model = svc.fit(features_patches_np_std, training_labels)\n",
        "  training_pred = np.array(model.predict(features_patches_np_std))\n",
        "  test_pred = np.array(model.predict(test_features_patches_np_std))\n",
        "  \n",
        "  print('C : ', C)\n",
        "  print('Training Overall Accuracy : ', accuracy_score(training_labels, training_pred))\n",
        "  print('Training Class Accuracy : ', confusion_matrix(training_labels, training_pred, normalize=\"true\").diagonal())\n",
        "  print('Test Overall Accuracy : ', accuracy_score(test_labels, test_pred))\n",
        "  print('Test Class Accuracy : ', confusion_matrix(test_labels, test_pred, normalize=\"true\").diagonal())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DTPYpLm0pSF"
      },
      "source": [
        "for C in [50, 100]:\n",
        "  for gamma in list(np.arange(0.1,1, 0.1)):\n",
        "    svc = SVC(kernel='rbf', class_weight='balanced', C=C, gamma=gamma)\n",
        "    model = svc.fit(features_patches_np_std, training_labels)\n",
        "    training_pred = np.array(model.predict(features_patches_np_std))\n",
        "    test_pred = np.array(model.predict(test_features_patches_np_std))\n",
        "    \n",
        "    print('C : ', C, 'gamma : ', gamma)\n",
        "    print('Training Overall Accuracy : ', accuracy_score(training_labels, training_pred))\n",
        "    print('Training Class Accuracy : ', confusion_matrix(training_labels, training_pred, normalize=\"true\").diagonal())\n",
        "    print('Test Overall Accuracy : ', accuracy_score(test_labels, test_pred))\n",
        "    print('Test Class Accuracy : ', confusion_matrix(test_labels, test_pred, normalize=\"true\").diagonal())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymT8Z45t10ao"
      },
      "source": [
        "# McNemar All image vs Partials for Linear\n",
        "\n",
        "svc = SVC(kernel='linear', class_weight='balanced', C=5.4)\n",
        "model_linear = svc.fit(features_std, training_labels)\n",
        "training_pred_linear = np.array(model_linear.predict(features_std))\n",
        "test_pred_linear = np.array(model_linear.predict(features_test_std))\n",
        "\n",
        "\n",
        "\n",
        "svc = SVC(kernel='linear', class_weight='balanced', C=5.2)\n",
        "model_linear_partial = svc.fit(features_patches_np_std, training_labels)\n",
        "training_pred_linear_partial = np.array(model_linear_partial.predict(features_patches_np_std))\n",
        "test_pred_linear_partial = np.array(model_linear_partial.predict(test_features_patches_np_std))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sZTxQEa41mz",
        "outputId": "ba43999c-9967-4425-a174-ecb823747234"
      },
      "source": [
        "for i in range(4):\n",
        "  table = contingency(training_labels, training_pred_linear, training_pred_linear_partial, i)\n",
        "  result = mcnemar(table, exact=False, correction=True)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pvalue      1.0\n",
            "statistic   0.0\n",
            "pvalue      0.4496917979688908\n",
            "statistic   0.5714285714285714\n",
            "pvalue      0.6830913983096086\n",
            "statistic   0.16666666666666666\n",
            "pvalue      0.47950012218695337\n",
            "statistic   0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDdn8JwA47kF",
        "outputId": "63d4fe9c-ab8f-4fd5-cec9-5ff9ff691ed0"
      },
      "source": [
        "for i in range(4):\n",
        "  table = contingency(test_labels, test_pred_linear, test_pred_linear_partial, i)\n",
        "  result = mcnemar(table, exact=False, correction=True)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pvalue      0.1456100953968629\n",
            "statistic   2.1176470588235294\n",
            "pvalue      0.18242243945173198\n",
            "statistic   1.7777777777777777\n",
            "pvalue      1.0\n",
            "statistic   0.0\n",
            "pvalue      0.37109336952269756\n",
            "statistic   0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axfVGyQu5SNV"
      },
      "source": [
        "# McNemar All image vs Partials for RBF\n",
        "\n",
        "svc = SVC(kernel='rbf', class_weight='balanced', C=10, gamma=0.1)\n",
        "model_rbf = svc.fit(features_std, training_labels)\n",
        "training_pred_rbf = np.array(model_rbf.predict(features_std))\n",
        "test_pred_rbf = np.array(model_rbf.predict(features_test_std))\n",
        "\n",
        "\n",
        "\n",
        "svc = SVC(kernel='rbf', class_weight='balanced', C=50, gamma=0.1)\n",
        "model_rbf_partial = svc.fit(features_patches_np_std, training_labels)\n",
        "training_pred_rbf_partial = np.array(model_rbf_partial.predict(features_patches_np_std))\n",
        "test_pred_rbf_partial = np.array(model_rbf_partial.predict(test_features_patches_np_std))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq59S-lw5s5m",
        "outputId": "c3bc99a4-06c9-4a13-b94e-5a4ad1134eb4"
      },
      "source": [
        "for i in range(4):\n",
        "  table = contingency(training_labels, training_pred_rbf, training_pred_rbf_partial, i)\n",
        "  result = mcnemar(table, exact=False, correction=True)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pvalue      0.2672574931543847\n",
            "statistic   1.2307692307692308\n",
            "pvalue      0.13057001811573693\n",
            "statistic   2.2857142857142856\n",
            "pvalue      1.0\n",
            "statistic   0.0\n",
            "pvalue      1.0\n",
            "statistic   0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8zxX5kk5uEI",
        "outputId": "55d66153-010e-40a2-91bb-aa6496ded83b"
      },
      "source": [
        "for i in range(4):\n",
        "  table = contingency(test_labels, test_pred_rbf, test_pred_rbf_partial, i)\n",
        "  result = mcnemar(table, exact=False, correction=True)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pvalue      0.7892680261342813\n",
            "statistic   0.07142857142857142\n",
            "pvalue      1.0\n",
            "statistic   0.0\n",
            "pvalue      1.0\n",
            "statistic   0.0\n",
            "pvalue      0.0\n",
            "statistic   inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/stats/contingency_tables.py:1336: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  statistic = (np.abs(n1 - n2) - corr)**2 / (1. * (n1 + n2))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}